db_tidy %>% count(word, sort = TRUE) %>% head(20)
# contar tokens en base
db_tidy <- db_tidy[!(db_tidy$word=="https" | db_tidy$word=="t.co"), ]
db_tidy %>% count(word, sort = TRUE) %>% head(20)
# hacer tabla para contar autores de tweets
table(base_train$name)
p_load("stopwords", "stringi", "tm", "rvest")
rm(list=ls())
#### **Instalar/llamar las librerías de la clase**
require(pacman)
p_load(tidyverse,rio,glue,
hexbin,
patchwork,vip, ## plot:
ggrepel, ## plot: geom_text_repel
stringi,tidytext,stopwords, ## text-data
tidymodels,finetune)
# set wd
setwd('C:/Users/Diego/OneDrive/Documents/GitHub/BD-ML---PS4/scripts/diego')
# cargar bases
base_train <- read.csv('C:data/train.csv')
base_test <- read.csv('C:data/test.csv')
glimpse(base_train)
# hacer tabla para contar autores de tweets
table(base_train$name)
p_load("stopwords", "stringi", "tm", "rvest")
# Creamos una lista con todos los stopwords en español
stopwords_español <- stopwords::stopwords("es", source = "snowball")
# Eliminamos los acentos de los stopwords
stopwords_español <- stri_trans_general(str = stopwords_español, id = "Latin-ASCII")
### 1. Normalización de tweets
# Eliminamos los acentos
base_train$text <- stri_trans_general(str = base_train$text, id = "Latin-ASCII")
# Ponemos el texto en minúscula
base_train$text <- tolower(base_train$text)
# Reemplazamos todos los caracteres no alfanumericos con un espacio
base_train$text <- str_replace_all(base_train$text, "[^[:alnum:]]", " ")
# Eliminamos los números
base_train$text <- gsub('[[:digit:]]+', '', base_train$text)
# Quitamos stopwords
base_train$text <- removeWords(base_train$text, stopwords_español)
# Eliminamos todos los espacios extras
base_train$text <- gsub("\\s+", " ", str_trim(base_train$text))
### 2. FUNCIÓN LEMMATIZACION
# crear una función para lematizar
# crear funcion de lemmatization: quitar sufijos y prefijos
lematiza = function(frase){
# Se reemplazan los espacios con +
query <- gsub(" ", "+", frase)
url_base <- "https://www.lenguaje.com/cgi-bin/lema.exe?edition_field="
url_final <- paste0(url_base, query,"&B1=Lematizar")
lemma <- read_html(url_final, encoding = "latin1") %>%
html_nodes('div') %>%
tail(1) %>%
html_nodes("li") %>%
html_text2() %>%
tail(1)
# lemma <- read_html(url_final, encoding = "latin1") %>%
#   html_node(css = "div div div div div li") %>%
#   html_text()
# lemma <- gsub("La palabra:", "", lemma)
# lemma <- gsub("tiene los siguientes lemas:", "", lemma)
# error <- "\r\n     Palabra no encontrada\r\n     Palabra no encontrada"
# lemma <- ifelse(lemma == error, frase, lemma)
if (length(lemma) == 0) {
return(frase)
} else {
lemma <- str_split(lemma, "\\n")[[1]][1]
return(lemma)
}
}
# ejemplo de la funcion
lematiza('encontramos')
# Para evitar doble computación vamos a crear un diccionario de palabras con su respectiva lematización
# 3. TOKENIZAR TEXTO: Tokenizaremos tweets
p_load(tidytext)
# crear ID de cada tweet
base_train$id <- 1:nrow(base_train)
# generar tokens a partir de tweets (texto)
tidy_tweets <- base_train %>%
unnest_tokens(output = token, input = text)
# 4. Generar modelo pre-entrenado
# Usando el comando automático de R con una aproximación más eficiente
p_load(udpipe)
# Creamos el id de cada documento
base_train$id <- paste0("tweet", 1:nrow(base_train))
# Descargamos el modelo pre-entrenado
udmodel <- udpipe_download_model(language = "spanish")
# extraer el file_model del modelo
# modelo <- udpipe_load_model(file = udmodel$file_model)
file_model <- udmodel$file_model
# crear modelo udpipe
modelo <- udpipe_load_model(file = file_model)
# extraer textp
x <- udpipe_annotate(modelo, x = base_train$text)
# convertir a data.frame
tidy_tweets <- as.data.frame(x)
### 2. FUNCIÓN LEMMATIZACION
# crear una función para lematizar
# crear funcion de lemmatization: quitar sufijos y prefijos
lematiza = function(frase){
# Se reemplazan los espacios con +
query <- gsub(" ", "+", frase)
url_base <- "https://www.lenguaje.com/cgi-bin/lema.exe?edition_field="
url_final <- paste0(url_base, query,"&B1=Lematizar")
lemma <- read_html(url_final, encoding = "latin1") %>%
html_nodes('div') %>%
tail(1) %>%
html_nodes("li") %>%
html_text2() %>%
tail(1)
# lemma <- read_html(url_final, encoding = "latin1") %>%
#   html_node(css = "div div div div div li") %>%
#   html_text()
# lemma <- gsub("La palabra:", "", lemma)
# lemma <- gsub("tiene los siguientes lemas:", "", lemma)
# error <- "\r\n     Palabra no encontrada\r\n     Palabra no encontrada"
# lemma <- ifelse(lemma == error, frase, lemma)
if (length(lemma) == 0) {
return(frase)
} else {
lemma <- str_split(lemma, "\\n")[[1]][1]
return(lemma)
}
}
# ejemplo de la funcion
lematiza('encontramos')
## TOKENS: word=token
# Generar una columna con los tokens, aplanando el tibble en un-token por fila.
# word = token
db_tidy <- base_train %>%
unnest_tokens(output = word, input = text) %>%
anti_join(get_stopwords("es"),"word")
# contar tokens en base
db_tidy <- db_tidy[!(db_tidy$word=="https" | db_tidy$word=="t.co"), ]
db_tidy %>% count(word, sort = TRUE) %>% head(20)
## n palabras con mayor frecuencia absoluta
# contar word=token
# filtrar tokens en character
top_words <- db_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(0:10)) %>%
slice_max(n, n = 100) %>%
pull(word)%>%head(10)
top_words
subset(base_train,id=="DgJXrP7JZuQS7trOVPj4xg==")$text
subset(db_tidy,id=="DgJXrP7JZuQS7trOVPj4xg==")$text
subset(base_train,id=="DgJXrP7JZuQS7trOVPj4xg==")$text
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
db_tidy %>% count(word, sort = TRUE) %>% head(20)
## n palabras con mayor frecuencia absoluta
# contar word=token
# filtrar tokens en character
top_words <- db_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(0:10)) %>%
slice_max(n, n = 100) %>%
pull(word)%>%head(10)
top_words
subset(base_train,id=="DgJXrP7JZuQS7trOVPj4xg==")$text
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
count_words %>% head(20)
## TOKENS: word=token
# Generar una columna con los tokens, aplanando el tibble en un-token por fila.
# word = token
db_tidy <- base_train %>%
unnest_tokens(output = word, input = text) %>%
anti_join(get_stopwords("es"),"word")
View(db_tidy)
rm(list=ls())
# cargar bases
base_train <- read.csv('C:data/train.csv')
base_test <- read.csv('C:data/test.csv')
### LDA
glimpse(base_train)
# hacer tabla para contar autores de tweets
table(base_train$name)
p_load("stopwords", "stringi", "tm", "rvest")
# hacer pre procesamiento de texto preparalo para LDA
# Creamos una lista con todos los stopwords en español
stopwords_español <- stopwords::stopwords("es", source = "snowball")
# Eliminamos los acentos de los stopwords
stopwords_español <- stri_trans_general(str = stopwords_español, id = "Latin-ASCII")
### 1. Normalización de tweets
# Eliminamos los acentos
base_train$text <- stri_trans_general(str = base_train$text, id = "Latin-ASCII")
# Ponemos el texto en minúscula
base_train$text <- tolower(base_train$text)
# Reemplazamos todos los caracteres no alfanumericos con un espacio
base_train$text <- str_replace_all(base_train$text, "[^[:alnum:]]", " ")
# Eliminamos los números
base_train$text <- gsub('[[:digit:]]+', '', base_train$text)
# Quitamos stopwords
base_train$text <- removeWords(base_train$text, stopwords_español)
# Eliminamos todos los espacios extras
base_train$text <- gsub("\\s+", " ", str_trim(base_train$text))
### 2. FUNCIÓN LEMMATIZACION
# crear una función para lematizar
# crear funcion de lemmatization: quitar sufijos y prefijos
lematiza = function(frase){
# Se reemplazan los espacios con +
query <- gsub(" ", "+", frase)
url_base <- "https://www.lenguaje.com/cgi-bin/lema.exe?edition_field="
url_final <- paste0(url_base, query,"&B1=Lematizar")
lemma <- read_html(url_final, encoding = "latin1") %>%
html_nodes('div') %>%
tail(1) %>%
html_nodes("li") %>%
html_text2() %>%
tail(1)
# lemma <- read_html(url_final, encoding = "latin1") %>%
#   html_node(css = "div div div div div li") %>%
#   html_text()
# lemma <- gsub("La palabra:", "", lemma)
# lemma <- gsub("tiene los siguientes lemas:", "", lemma)
# error <- "\r\n     Palabra no encontrada\r\n     Palabra no encontrada"
# lemma <- ifelse(lemma == error, frase, lemma)
if (length(lemma) == 0) {
return(frase)
} else {
lemma <- str_split(lemma, "\\n")[[1]][1]
return(lemma)
}
}
# ejemplo de la funcion
lematiza('encontramos')
base_train$text[1]
base_train$text[2]
## TOKENS: word=token
# Generar una columna con los tokens, aplanando el tibble en un-token por fila.
# word = token
db_tidy <- base_train %>%
unnest_tokens(output = word, input = text) %>%
anti_join(get_stopwords("es"),"word")
db_tidy %>% count(word, sort = TRUE) %>% head(20)
# contar tokens en base
db_tidy <- db_tidy[!(db_tidy$word=="https" | db_tidy$word=="t.co"), ]
db_tidy %>% count(word, sort = TRUE) %>% head(20)
db_tidy %>% count(word, sort = TRUE) %>% head(50)
# contar tokens en base
db_tidy <- db_tidy[!(db_tidy$word=="https" | db_tidy$word=="t.co" | db_tidy$word=="co" | db_tidy$word=="t"), ]
db_tidy %>% count(word, sort = TRUE) %>% head(50)
## n palabras con mayor frecuencia absoluta
# contar word=token
# filtrar tokens en character
top_words <- db_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(0:10)) %>%
slice_max(n, n = 100) %>%
pull(word)%>%head(10)
top_words
subset(base_train,id=="DgJXrP7JZuQS7trOVPj4xg==")$text
# count_words sobre la base db_tidy
# contar (word=token, name)
# complete (word=token, name, fill = list(n = 0)) - expandir todas las combinaciones
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
count_words %>% head(20)
word_freqs <- count_words %>%
group_by(name) %>%
mutate(name_sum = sum(n),
proportion = n / name_sum) %>%
ungroup() %>%
filter(word %in% top_words)
word_freqs
View(word_freqs)
word_model <- word_freqs %>%
nest(data = c(name, n, name_sum, proportion)) %>%
mutate(model = map(.x = data,
.f = ~ glm(cbind(n, name_sum) ~ name, data = . , family = "binomial")),
model = map(model, tidy)) %>%
unnest(model) %>%
filter(term == "name") %>%
mutate(p.value = p.adjust(p.value)) %>%
arrange(-estimate)
word_model
word_model
### plot estimate vs p-value
word_model %>%
ggplot(aes(estimate, p.value)) +
geom_vline(xintercept = 0, lty = 2, alpha = 0.7, color = "gray50") +
geom_point(color = "midnightblue", alpha = 0.8, size = 2.5) +
scale_y_log10() +
geom_text_repel(aes(label = word))
word_model <- word_freqs %>%
nest(data = c(name, n, name_sum, proportion)) %>%
mutate(model = map(.x = data,
.f = ~ glm(cbind(n, name_sum) ~ name, data = . , family = "binomial")),
model = map(model, tidy)) %>%
unnest(model) %>%
filter(term == "name") %>%
mutate(p.value = p.adjust(p.value)) %>%
arrange(-estimate)
word_model
## generar vector con palabras que mas aumentan o disminuyen con el nombre
higher_words <- word_model %>%
filter(p.value < 0.05) %>%
slice_max(estimate, n = 12) %>%
pull(word)
lower_words <- word_model %>%
filter(p.value < 0.05) %>%
slice_max(-estimate, n = 12) %>%
pull(word)
### plot lower_words
# base word_freqs
word_freqs %>%
filter(word %in% lower_words) %>%
ggplot(aes(price, proportion, color = word)) +
geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(vars(word), scales = "free_y") + theme_light()
## plot higher_words
word_freqs %>%
filter(word %in% higher_words) %>%
ggplot(aes(price, proportion, color = word)) +
geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
facet_wrap(vars(word), scales = "free_y") + theme_light()
## TOKENS: word=token
# Generar una columna con los tokens, aplanando el tibble en un-token por fila.
# word = token
db_tidy <- base_train %>%
unnest_tokens(output = word, input = text) %>%
anti_join(get_stopwords("es"),"word")
# contar tokens en base
db_tidy <- db_tidy[!(db_tidy$word=="https" | db_tidy$word=="t.co" | db_tidy$word=="co" | db_tidy$word=="t"), ]
db_tidy %>% count(word, sort = TRUE) %>% head(50)
## n palabras con mayor frecuencia absoluta
# contar word=token
# filtrar tokens en character
top_words <- db_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(0:10)) %>%
slice_max(n, n = 100) %>%
pull(word)%>%head(10)
top_words
subset(base_train,id=="DgJXrP7JZuQS7trOVPj4xg==")$text
# count_words sobre la base db_tidy
# contar (word=token, name)
# complete (word=token, name, fill = list(n = 0)) - expandir todas las combinaciones
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
count_words %>% head(20)
word_freqs <- count_words %>%
group_by(name) %>%
mutate(name_sum = sum(n),
proportion = n / name_sum) %>%
ungroup() %>%
filter(word %in% top_words)
word_freqs
word_model <- word_freqs %>%
nest(data = c(name, n, name_sum, proportion)) %>%
mutate(model = map(.x = data,
.f = ~ glm(cbind(n, name_sum) ~ name, data = . , family = "binomial")),
model = map(model, tidy)) %>%
unnest(model) %>%
filter(term == "name") %>%
mutate(p.value = p.adjust(p.value)) %>%
arrange(-estimate)
## TOKENS: word=token
# Generar una columna con los tokens, aplanando el tibble en un-token por fila.
# word = token
db_tidy <- base_train %>%
unnest_tokens(output = word, input = text) %>%
anti_join(get_stopwords("es"),"word")
View(db_tidy)
# contar tokens en base
db_tidy <- db_tidy[!(db_tidy$word=="https" | db_tidy$word=="t.co" | db_tidy$word=="co" | db_tidy$word=="t"), ]
db_tidy %>% count(word, sort = TRUE) %>% head(50)
## n palabras con mayor frecuencia absoluta
# contar word=token
# filtrar tokens en character
top_words <- db_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(0:10)) %>%
slice_max(n, n = 100) %>%
pull(word)%>%head(10)
top_words
top_words
subset(base_train,id=="DgJXrP7JZuQS7trOVPj4xg==")$text
# count_words sobre la base db_tidy
# contar (word=token, name)
# complete (word=token, name, fill = list(n = 0)) - expandir todas las combinaciones
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
count_words %>% head(20)
db_tidy %>% count(word, sort = TRUE) %>% head(50)
## n palabras con mayor frecuencia absoluta
# contar word=token
# filtrar tokens en character
top_words <- db_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(0:10)) %>%
slice_max(n, n = 100) %>%
pull(word)%>%head(10)
top_words
top_words
count_words %>% head(20)
# count_words sobre la base db_tidy
# contar (word=token, name)
# complete (word=token, name, fill = list(n = 0)) - expandir todas las combinaciones
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
count_words %>% head(20)
top_words
subset(base_train,id=="DgJXrP7JZuQS7trOVPj4xg==")$text
# count_words sobre la base db_tidy
# contar (word=token, name)
# complete (word=token, name, fill = list(n = 0)) - expandir todas las combinaciones
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
count_words %>% head(20)
## n palabras con mayor frecuencia absoluta
# contar word=token
# filtrar tokens en character
top_words <- db_tidy %>%
count(word, sort = TRUE) %>%
filter(!word %in% as.character(0:10)) %>%
slice_max(n, n = 100) %>%
pull(word)%>%head(10)
top_words
# count_words sobre la base db_tidy
# contar (word=token, name)
# complete (word=token, name, fill = list(n = 0)) - expandir todas las combinaciones
count_words <- db_tidy %>%
count(word, name) %>%
complete(word, name, fill = list(n = 0)) ## expandir todas las posibles combinaciones
count_words %>% head(20)
word_freqs <- count_words %>%
group_by(name) %>%
mutate(name_sum = sum(n),
proportion = n / name_sum) %>%
ungroup() %>%
filter(word %in% top_words)
# Para evitar doble computación vamos a crear un diccionario de palabras con su respectiva lematización
# 3. TOKENIZAR TEXTO: Tokenizaremos tweets
p_load(tidytext)
# crear ID de cada tweet
base_train$id <- 1:nrow(base_train)
# generar tokens a partir de tweets (texto)
tidy_tweets <- base_train %>%
unnest_tokens(output = token, input = text)
# 4. Generar modelo pre-entrenado
# Usando el comando automático de R con una aproximación más eficiente
p_load(udpipe)
# Creamos el id de cada documento
base_train$id <- paste0("tweet", 1:nrow(base_train))
# Descargamos el modelo pre-entrenado
udmodel <- udpipe_download_model(language = "spanish")
# extraer el file_model del modelo
# modelo <- udpipe_load_model(file = udmodel$file_model)
file_model <- udmodel$file_model
# crear modelo udpipe
modelo <- udpipe_load_model(file = file_model)
# extraer texto
x <- udpipe_annotate(modelo, x = base_train$text)
# convertir a data.frame
tidy_tweets <- as.data.frame(x)
tidy_tweets[tidy_tweets$doc_id == "doc101", "lemma"]
View(tidy_tweets)
word_count <- tidy_tweets %>%
group_by(doc_id) %>%
count(lemma) %>%
ungroup()
num_tweets <- nrow(base_train)
num_tweets
word_count2 <- word_count %>%
left_join(
word_count %>%
count(lemma) %>%
mutate(filtro = !((n <= num_tweets*0.05) | (n >= num_tweets*0.5))) %>%
select(-n)
) %>%
filter(filtro) %>%
select(-filtro)
# Eliminamos los acentos
word_count2$lemma <- stri_trans_general(str = word_count2$lemma, id = "Latin-ASCII")
# crear filtro de stopwords para quitar de word_count2
filtro <- !(word_count2$lemma %in% stopwords_español)
word_count2 <- word_count2[filtro, ]
#### crear matriz DTM
p_load(tm, tidytext)
word_count <- tidy_tweets %>%
group_by(doc_id) %>%
count(lemma) %>%
ungroup()
View(word_count)
num_tweets <- nrow(base_train)
word_count2 <- word_count %>%
left_join(
word_count %>%
count(lemma) %>%
mutate(filtro = !((n <= num_tweets*0.05) | (n >= num_tweets*0.5))) %>%
select(-n)
) %>%
filter(filtro) %>%
select(-filtro)
# Eliminamos los acentos
word_count2$lemma <- stri_trans_general(str = word_count2$lemma, id = "Latin-ASCII")
# Quitamos stopwords
# crear filtro de stopwords para quitar de word_count2
filtro <- !(word_count2$lemma %in% stopwords_español)
word_count2 <- word_count2[filtro, ]
